{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add the clean movie function that takes in the argument, \"movie\"\n",
    "def clean_movie(movie):\n",
    "    \n",
    "    movie = dict(movie)\n",
    "    alt_titles = {}\n",
    "    \n",
    "    # combine alternate titles into one list\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        \n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)\n",
    "    \n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "    \n",
    "    # combining similar columns names\n",
    "    def change_col_name(old_name, new_name):\n",
    "        \n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "        \n",
    "    change_col_name('Adaptation by', 'Written by')\n",
    "    change_col_name('Country of origin', 'Country')\n",
    "    change_col_name('Directed by', 'Director')\n",
    "    change_col_name('Distributed by', 'Distributor')\n",
    "    change_col_name('Edited by', 'Editor(s)')\n",
    "    change_col_name('Length', 'Running time')\n",
    "    change_col_name('Original release', 'Release date')\n",
    "    change_col_name('Music by', 'Composer(s)')\n",
    "    change_col_name('Produced by', 'Producer(s)')\n",
    "    change_col_name('Producer', 'Producer(s)')\n",
    "    change_col_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_col_name('Productioncompany ', 'Production company(s)')\n",
    "    change_col_name('Released', 'Release Date')\n",
    "    change_col_name('Release Date', 'Release date')\n",
    "    change_col_name('Screen story by', 'Written by')\n",
    "    change_col_name('Screenplay by', 'Written by')\n",
    "    change_col_name('Story by', 'Written by')\n",
    "    change_col_name('Theme music composer', 'Music by')\n",
    "    \n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a function that takes in three arguments\n",
    "# Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "def etl(x, y, z):\n",
    "    \n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames\n",
    "    kaggle_metadata_df = pd.read_csv(kaggle_file, low_memory=False)\n",
    "    ratings = pd.read_csv(ratings_file)\n",
    "\n",
    "    # Open and read the Wikipedia data JSON file\n",
    "    with open(wiki_file, mode='r') as file:\n",
    "        raw = json.load(file)\n",
    "    \n",
    "    # list comprehension to filter out TV shows\n",
    "    cleaned_wiki = [row for row in raw if ('Director' in row \\\n",
    "                                           or 'Directed by' in row) \\\n",
    "                                           and 'Television series' not in row \\\n",
    "                                           or 'No. of episodes' not in row \\\n",
    "                                           or 'No. of seasons' not in row \\\n",
    "                                           or 'Seasons' not in row]\n",
    "    \n",
    "\n",
    "    # list comprehension to iterate through cleaned wiki movies list and call clean_movie function on each movie\n",
    "    clean_list = [clean_movie(x) for x in cleaned_wiki]\n",
    "\n",
    "    # read in the cleaned movies list as a DataFrame\n",
    "    wiki_movies_df = pd.DataFrame(clean_list)\n",
    "\n",
    "    # try-except block to catch errors while extracting the IMDb ID using a regex string and drop any duplicates \n",
    "    try:\n",
    "        wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "        wiki_movies_df.drop_duplicates(subset=\"imdb_id\", inplace=True)\n",
    "    except Exception:\n",
    "        print(\"Oh fun, an error.\" (Exception))\n",
    "\n",
    "    #  list comprehension to keep columns that don't have null values from wiki_movies_df DataFrame\n",
    "    keep_columns = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "\n",
    "    # variable to hold the non-null values from the “Box office” column\n",
    "    box_office = wiki_movies_df['Box office'].dropna()\n",
    "    \n",
    "    # convert box office data to string values using lambda function\n",
    "    # if data is stored as a list use join function to concatenate into a string with ' ' between each\n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    # regex to match the six elements of \"form_one\" of the box office data\n",
    "    form_1 = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    \n",
    "    # regex to match the three elements of \"form_two\" of the box office data\n",
    "    form_2 = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "\n",
    "    # add the parse_dollars function\n",
    "    def parse_dollars(s):\n",
    "        \n",
    "        # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "    \n",
    "        # if input is of the form $###.# million\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "        \n",
    "            # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "        \n",
    "            # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "        \n",
    "            # return value\n",
    "            return value\n",
    "    \n",
    "        # if input is of the form $###.# billion\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "        \n",
    "            # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "        \n",
    "            # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "        \n",
    "            # return value\n",
    "            return value\n",
    "    \n",
    "        # if input is of the form $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "        \n",
    "            # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)\n",
    "        \n",
    "            # convert to float\n",
    "            value = float(s)\n",
    "        \n",
    "            # return value\n",
    "            return value\n",
    "    \n",
    "        # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "        \n",
    "    # clean box office column in wiki_movies_df DataFrame\n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_1}|{form_2})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    # clean budget column in wiki_movies_df DataFrame\n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True) # remove values between $ and hypen\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '') # remove citations\n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_1}|{form_2})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    # clean release date column in wiki_movies_df DataFrame\n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]?\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[0123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "    release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n",
    "    \n",
    "    # clean running time column in wiki_movies_df DataFrame\n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "    # clean Kaggle metadata by dropping 'adult' films and converting data types\n",
    "    kaggle_metadata_df = kaggle_metadata_df[kaggle_metadata_df['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    kaggle_metadata_df['budget'] = kaggle_metadata_df['budget'].astype(int)\n",
    "    kaggle_metadata_df['id'] = pd.to_numeric(kaggle_metadata_df['id'], errors='raise')\n",
    "    kaggle_metadata_df['popularity'] = pd.to_numeric(kaggle_metadata_df['popularity'], errors='raise')\n",
    "    kaggle_metadata_df['release_date_kaggle'] = pd.to_datetime(kaggle_metadata_df['release_date'])\n",
    "    kaggle_metadata_df['video'] = kaggle_metadata_df['video'] == 'True'\n",
    "\n",
    "    # merge the two DataFrames into movies DataFrame\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata_df, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    # drop unnecessary columns from movies DataFrame\n",
    "    movies_df.drop(columns=['title_wiki','Language','Production company(s)'], inplace=True)\n",
    "\n",
    "    # add function to fill in the missing Kaggle data\n",
    "    def fill_missing(df, kaggle_column, wiki_column):\n",
    "    \n",
    "        df[kaggle_column] = df.apply(lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column], axis=1)\n",
    "    \n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "\n",
    "    # call the fill_missing function DataFrame and columns as arguments\n",
    "    fill_missing(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing(movies_df, 'revenue', 'box_office')\n",
    "\n",
    "    # drop video column\n",
    "    movies_df.drop(columns=['video'], inplace=True)\n",
    "\n",
    "    # reorder columns for readability\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Written by','Music by','Based on']]\n",
    "    \n",
    "    # rename columns in movies DataFrame for consistency\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Written by':'writers',\n",
    "                  'Music by':'composers',\n",
    "                  'Based on':'based_on'}, \n",
    "                     axis='columns', inplace=True)\n",
    "\n",
    "    # transform and merge the ratings DataFrame\n",
    "    # groupby movieId & rating, rename userId as count, pivot on movieId as index \n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1) \\\n",
    "                .pivot(index='movieId',columns='rating', values='count')\n",
    "    # rename columns\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    # merge movies_df & rating_counts\n",
    "    movie_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    # fill any blanks with 0\n",
    "    movie_ratings_df[rating_counts.columns] = movie_ratings_df[rating_counts.columns].fillna(0)\n",
    "    \n",
    "    # create connection string\n",
    "    db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5433/movie_data\"\n",
    "    #create engine\n",
    "    engine = create_engine(db_string)\n",
    "    # add DataFrame to SQL database\n",
    "    movies_df.to_sql(name='movies', con=engine, if_exists='replace')\n",
    "    \n",
    "    # create a variable for the number of rows imported\n",
    "    rows_imported = 0\n",
    "\n",
    "    # get the start_time from time.time()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for data in pd.read_csv(f'{file_dir}ratings.csv', chunksize=1000000):\n",
    "\n",
    "        # print out the range of rows that are being imported\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "        # increment the number of rows imported by the chunksize\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # print that the rows have finished importing along with elapsed time\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the path to file directory and variables for the three files\n",
    "file_dir = \"C:\\\\Users\\\\Frank Bucalo\\\\Desktop\\\\classwork\\\\Movies-ETL\\\\\"\n",
    "# The Wikipedia data\n",
    "wiki_file = f'{file_dir}wikipedia-movies.json'\n",
    "# The Kaggle metadata\n",
    "kaggle_file = f'{file_dir}movies_metadata.csv'\n",
    "# The MovieLens rating data.\n",
    "ratings_file = f'{file_dir}ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\PythonData\\lib\\site-packages\\ipykernel_launcher.py:106: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Done. 30.329548120498657 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 58.83858513832092 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 88.95331358909607 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 118.40760803222656 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 145.9919455051422 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 176.54497170448303 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 205.55298566818237 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 235.4390001296997 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 274.8316102027893 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 311.17630434036255 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 343.3491988182068 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 374.2883985042572 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 407.31006121635437 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 437.8582684993744 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 470.5857207775116 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 504.0611717700958 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 535.2521913051605 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 567.636561870575 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 599.5142085552216 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 627.2300820350647 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 655.9695615768433 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 683.9811246395111 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 714.056319475174 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 742.3468050956726 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 770.8757178783417 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 800.4484324455261 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 801.312135219574 total seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "# pass the three variables into extract, transform, load function\n",
    "etl(wiki_file, kaggle_file, ratings_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
